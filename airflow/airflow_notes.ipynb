{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Airflow Overview"
      ],
      "metadata": {
        "id": "Raozxy8EfqYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## O que é o Apache Airflow?\n"
      ],
      "metadata": {
        "id": "nCaxlTwkfrQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Apache Airflow é uma plataforma de orquestração de workflows open source, criada pelo Airbnb em 2015 e mantida pela Fundação Apache. Desenvolvido em Python, o Airflow permite a criação, agendamento e monitoramento de workflows de dados. Ele é extensível, o que significa que pode ser adaptado e expandido para diferentes casos de uso através de plugins e integrações."
      ],
      "metadata": {
        "id": "TOJVdEZPgAUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Como o Apache Airflow Funciona?"
      ],
      "metadata": {
        "id": "z8HYKo5sgJH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Airflow utiliza DAGs (Directed Acyclic Graphs, ou Grafos Acíclicos Direcionados) para definir seus workflows. Uma DAG é essencialmente um pipeline que descreve um conjunto de tarefas que devem ser executadas, suas dependências e a ordem de execução. Essas tarefas, chamadas de Tasks, são instâncias de Operators."
      ],
      "metadata": {
        "id": "RA9jarXUgLT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Componentes Principais:\n"
      ],
      "metadata": {
        "id": "DV4IryPExCDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **DAG (Directed Acyclic Graph)**:\n",
        "   - É um grafo composto por vértices e arestas, onde as arestas têm direção e não formam ciclos. No contexto do Airflow, representa um pipeline de dados.\n"
      ],
      "metadata": {
        "id": "Jv8Wos6A0PYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Exemplo de DAG para demonstrar o funcionamento básico do Apache Airflow.\n",
        "\n",
        "Notas sobre o uso:\n",
        "\n",
        "- Esta DAG executa duas tarefas: uma usando o BashOperator para imprimir \"hello\" e outra tarefa Python que imprime \"airflow\".\n",
        "- A DAG será executada diariamente a partir de 2 de setembro de 2024.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from airflow.decorators import task\n",
        "\n",
        "# Definição da DAG\n",
        "with DAG(\n",
        "    dag_id=\"demo_dag\",\n",
        "    start_date=datetime(2024, 9, 2),\n",
        "    schedule_interval=\"0 0 * * *\",  # Executa diariamente à meia-noite\n",
        "    catchup=False,  # Evita a execução retroativa\n",
        "    tags=[\"demo\"],\n",
        ") as dag:\n",
        "\n",
        "    # Tarefa que imprime \"hello\" usando o BashOperator\n",
        "    print_hello = BashOperator(\n",
        "        task_id=\"print_hello\",\n",
        "        bash_command=\"echo hello\"\n",
        "    )\n",
        "\n",
        "    @task\n",
        "    def print_airflow():\n",
        "        print(\"airflow\")\n",
        "\n",
        "    # Definição da ordem de execução das tarefas\n",
        "    print_hello >> print_airflow()\n"
      ],
      "metadata": {
        "id": "cvOV0vFC0rCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Operators e Tasks**:\n",
        "   - **Operators** são os blocos de construção de um pipeline. Eles definem o que precisa ser executado, como transferências de dados ou execução de scripts.\n",
        "   - **Tasks** são instâncias de um Operator e representam uma unidade de trabalho que será executada no pipeline."
      ],
      "metadata": {
        "id": "9wyFWPDt0YSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Arquitetura**:\n",
        "   - **Metadata**: Armazena o estado do DAG e das tasks.\n",
        "   - **Scheduler**: Decide quando uma task deve ser executada.\n",
        "   - **Web UI**: Interface para monitoramento e gerenciamento das DAGs.\n",
        "   - **Executors**: Gerenciam a execução das tasks, podendo ser distribuídos para suportar alta disponibilidade e distribuição de carga.\n"
      ],
      "metadata": {
        "id": "awA8OXoU0cXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Tipos de Operators**:\n",
        "   - **Sensor**: Monitoram eventos, como a presença de um arquivo em um diretório.\n",
        "   - **Transfer**: Transferem dados entre sistemas.\n",
        "   - **Action**: Executam ações, como scripts Bash ou Python."
      ],
      "metadata": {
        "id": "-sVrGUPK0fbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Providers**:\n",
        "   - São integrações com outras ferramentas e serviços, como AWS, Databricks, MySQL e MongoDB, que permitem que o Airflow interaja diretamente com esses serviços."
      ],
      "metadata": {
        "id": "J5-mLBAQ0iK1"
      }
    }
  ]
}